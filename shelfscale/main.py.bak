"""
Main script demonstrating ShelfScale package usage with comprehensive data sources
with machine learning capabilities for continuous improvement
"""

import os
import pandas as pd
import numpy as np
import tabula
import PyPDF2
import re
import argparse
from fuzzywuzzy import process, fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random
import logging
from typing import List, Dict, Tuple, Optional, Any, Union

from shelfscale.data_sourcing.open_food_facts import OpenFoodFactsClient
from shelfscale.data_processing.weight_extraction import clean_weights
from shelfscale.data_processing.categorization import FoodCategorizer, clean_food_categories
from shelfscale.data_processing.cleaner import DataCleaner
from shelfscale.data_processing.transformation import normalize_weights, create_food_group_summary
from shelfscale.matching.algorithm import FoodMatcher
from shelfscale.utils.helpers import (
    load_data, 
    save_data, 
    split_data_by_group,
    get_path,
    extract_numeric_value
)
from shelfscale.utils.learning import (
    train_matcher_from_existing_data, 
    evaluate_matcher_performance,
    generate_training_data_from_matches,
    create_weight_predictions,
    apply_feedback_to_matches,
    load_existing_matches
)
from shelfscale.data_sourcing.pdf_extraction import PDFExtractor
from shelfscale.data_processing.raw_processor import RawDataProcessor, process_raw_data

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def extract_food_portion_data(pdf_path):
    """Extract data from Food Portion Sizes PDF"""
    print("Extracting data from Food Portion Sizes PDF...")
    
    try:
        # Extract tables from PDF
        tables = tabula.read_pdf(pdf_path, 
                                pages="12-114", 
                                stream=True, 
                                multiple_tables=True, 
                                guess=False, 
                                encoding="latin1",
                                pandas_options={"header": [0, 1, 2, 3]}
                            )
        
        # Combine tables into single DataFrame
        df = pd.DataFrame()
        for table in tables:
            df = pd.concat([df, table], ignore_index=True)
        
        # Clean up the DataFrame
        df.columns = ['Food_Name', 'Portion_Size', 'Weight_g', 'Notes']
        df = df.dropna(thresh=2)  # Drop rows with less than 2 non-NaN values
        
        print(f"  Extracted {len(df)} food portion records")
        return df
    except Exception as e:
        print(f"Error extracting data from PDF: {str(e)}")
        # Return empty DataFrame as fallback
        return pd.DataFrame(columns=['Food_Name', 'Portion_Size', 'Weight_g', 'Notes'])


def extract_fruit_veg_survey_data(pdf_path):
    """Extract data from Fruit and Vegetable survey PDF"""
    print("Extracting data from Fruit and Vegetable Survey PDF...")
    
    try:
        # Extract text from PDF
        extracted_data = {}
        with open(pdf_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            num_pages = len(pdf_reader.pages)
            
            # Extract data from pages with sample information
            start_page = 11
            end_page = min(1291, num_pages)
            
            for page_num in range(start_page - 1, end_page):
                try:
                    # Get the page
                    page = pdf_reader.pages[page_num]
                    # Extract text
                    page_text = page.extract_text()
                    # Store in dict
                    extracted_data[page_num + 1] = page_text
                except Exception as e:
                    print(f"  Warning: Error extracting text from page {page_num + 1}: {str(e)}")
                    continue
        
        # Parse extracted text into structured data
        samples = []
        
        # Regular expressions for key information
        sample_number_pattern = r"Composite Sample Number:\s*(\d+)"
        sample_name_pattern = r"Composite Sample Name:\s*(.*?)\n"
        weight_pattern = r"Pack size:\s*(.*?)\n"
        
        # Process each page
        for page_num, text in extracted_data.items():
            # Extract information using regex
            sample_number_match = re.search(sample_number_pattern, text)
            sample_name_match = re.search(sample_name_pattern, text)
            weight_match = re.search(weight_pattern, text)
            
            if sample_name_match:
                sample = {
                    'Page': page_num,
                    'Sample_Number': sample_number_match.group(1) if sample_number_match else None,
                    'Sample_Name': sample_name_match.group(1).strip() if sample_name_match else None,
                    'Pack_Size': weight_match.group(1).strip() if weight_match else None
                }
                samples.append(sample)
        
        # Create DataFrame
        df = pd.DataFrame(samples)
        print(f"  Extracted {len(df)} fruit and vegetable samples")
        return df
    except Exception as e:
        print(f"Error extracting data from fruit and vegetable survey PDF: {str(e)}")
        # Return empty DataFrame as fallback
        return pd.DataFrame(columns=['Page', 'Sample_Number', 'Sample_Name', 'Pack_Size'])


def load_mccance_widdowson_data(file_path):
    """Load McCance and Widdowson's food composition data"""
    print("Loading McCance and Widdowson's data...")
    try:
        if not os.path.exists(file_path):
            print(f"  Error: McCance and Widdowson data file not found: {file_path}")
            return pd.DataFrame()
        # Try different sheet names since the data format might vary
        sheet_names = ["1.2 Factors", "1.2_Factors", "Factors", "Food Composition"]
        df = None
        
        # First try to get all sheet names from the file
        try:
            available_sheets = pd.ExcelFile(file_path).sheet_names
            print(f"  Available sheets: {', '.join(available_sheets)}")
            
            # Add available sheets to our list to try
            for sheet in available_sheets:
                if sheet not in sheet_names:
                    sheet_names.append(sheet)
        except Exception as e:
            print(f"  Warning: Could not read sheet names: {str(e)}")
        
        # Try each sheet name until we find one that works
        for sheet in sheet_names:
            try:
                df = pd.read_excel(file_path, sheet_name=sheet)
                if len(df) > 0:
                    print(f"  Successfully loaded data from sheet: {sheet}")
                    break
            except Exception as e:
                print(f"  Could not load sheet '{sheet}': {str(e)}")
        
        if df is None or len(df) == 0:
            print("  Error: Could not load any data from the Excel file.")
            return pd.DataFrame()
        
        print(f"  Loaded {len(df)} food items")
        print(f"  Available columns: {', '.join(df.columns.tolist())}")
        
        # Check if data is valid
        non_null_counts = df.count()
        if non_null_counts.sum() == 0:
            print("  Warning: Data appears to be empty or corrupted (all values are NaN)")
            return pd.DataFrame()
            
        # Identify key columns that should have data
        key_cols = ['Food Name', 'Food Code', 'FoodName', 'Food_Name', 'Description']
        found_key_col = False
        
        for col in key_cols:
            if col in df.columns and df[col].notna().sum() > 0:
                found_key_col = True
                # Standardize the column name to 'Food Name'
                if col != 'Food Name':
                    df.rename(columns={col: 'Food Name'}, inplace=True)
                break
        
        if not found_key_col:
            print(f"  Warning: No key column with food names found. Available columns: {', '.join(df.columns)}")
        
        return df
    except Exception as e:
        print(f"Error loading McCance and Widdowson data: {str(e)}")
        return pd.DataFrame()


def match_datasets(main_df, secondary_df, main_col, secondary_col, threshold=70, additional_cols=None):
    """Match items between datasets using fuzzy matching with machine learning"""
    print(f"Matching datasets based on {main_col} and {secondary_col}...")
    
    # Try to load a trained matcher first for better results
    try:
        matcher = train_matcher_from_existing_data()
        logger.info("Using trained matcher for dataset matching")
    except Exception as e:
        logger.warning(f"Could not load trained matcher: {e}. Using default matcher.")
        matcher = FoodMatcher(similarity_threshold=threshold/100)
    
    # Set up additional matching columns
    additional_match_cols = None
    if additional_cols:
        additional_match_cols = [(col, col) for col in additional_cols if col in main_df.columns and col in secondary_df.columns]
    
    # Perform matching
    matches = matcher.match_datasets(
        main_df, 
        secondary_df, 
        main_col, 
        secondary_col,
        additional_match_cols=additional_match_cols
    )
    
    # Merge matched datasets
    merged_df = matcher.merge_matched_datasets(
        main_df,
        secondary_df,
        matches
    )
    
    print(f"  Found {len(merged_df)} matches with similarity > {threshold}%")
    return merged_df


def process_weight_info(df, weight_cols):
    """
    Process weight information from one or more columns
    
    Args:
        df: Input DataFrame
        weight_cols: Column(s) containing weight information
            Can be a string or list of strings
            
    Returns:
        DataFrame with processed weight information
    """
    # Make a copy
    weight_processed_df = df.copy()
    
    # Handle single column as string
    if isinstance(weight_cols, str):
        weight_cols = [weight_cols]
    
    # Try to find weight columns if none provided
    if not weight_cols or not any(col in df.columns for col in weight_cols):
        # Look for columns that might contain weight information
        potential_weight_cols = []
        for col in df.columns:
            col_lower = col.lower()
            if any(term in col_lower for term in ['weight', 'size', 'portion', 'pack', 'g)', 'kg)', 'g', 'kg']):
                potential_weight_cols.append(col)
        
        if potential_weight_cols:
            print(f"  No valid weight columns provided, using detected columns: {', '.join(potential_weight_cols)}")
            weight_cols = potential_weight_cols
        else:
            print("  No weight columns found in the DataFrame")
            # Create empty weight column to ensure consistent output structure
            weight_processed_df['Normalized_Weight'] = np.nan
            return weight_processed_df
    
    # Create a normalized weight column
    weight_processed_df['Normalized_Weight'] = np.nan
    weight_processed_df['Weight_Unit'] = None
    weight_processed_df['Weight_Source_Col'] = None
    
    # Count successful extractions
    total_count = 0
    success_count = 0
    extraction_stats = {}
    
    # Process each weight column
    for weight_col in weight_cols:
        if weight_col not in weight_processed_df.columns:
            print(f"  Warning: Weight column '{weight_col}' not found in DataFrame")
            continue
        
        # Initialize stats for this column
        extraction_stats[weight_col] = {'total': 0, 'success': 0}
        
        # Check if the column has any non-null values
        if weight_processed_df[weight_col].isna().all():
            print(f"  Warning: Weight column '{weight_col}' contains only null values")
            continue
            
        # Iterate through rows
        for idx, row in weight_processed_df.iterrows():
            # Skip if we already have a normalized weight from a previous column
            if not pd.isna(weight_processed_df.loc[idx, 'Normalized_Weight']):
                continue
                
            # Skip if empty
            if pd.isna(row[weight_col]):
                continue
                
            extraction_stats[weight_col]['total'] += 1
            total_count += 1
            weight_text = str(row[weight_col])
            
            # Skip empty or "nan" values
            if not weight_text or weight_text.lower() == "nan":
                continue
                
            # Extract weight value using helper function
            try:
                weight, unit = extract_numeric_value(weight_text, return_unit=True)
                
                # If successful, add to normalized column
                if weight is not None and weight > 0:
                    weight_processed_df.loc[idx, 'Normalized_Weight'] = weight
                    weight_processed_df.loc[idx, 'Weight_Unit'] = unit if unit else 'g'
                    weight_processed_df.loc[idx, 'Weight_Source_Col'] = weight_col
                    extraction_stats[weight_col]['success'] += 1
                    success_count += 1
            except Exception as e:
                logger.warning(f"Error extracting weight from '{weight_text}': {e}")
    
    # Calculate success rate
    success_rate = (success_count / max(1, total_count)) * 100
    print(f"  Successfully extracted {success_count} weights out of {total_count} entries ({success_rate:.2f}%)")
    
    # Print per-column statistics
    for col, stats in extraction_stats.items():
        if stats['total'] > 0:
            col_success_rate = (stats['success'] / stats['total']) * 100
            print(f"    {col}: {stats['success']}/{stats['total']} ({col_success_rate:.2f}%)")
    
    return weight_processed_df


def extract_numeric_value(text, return_unit=False):
    """
    Extract numeric value from text, optionally returning the unit
    
    Args:
        text: Text string containing a numeric value
        return_unit: Whether to return the unit as well
        
    Returns:
        If return_unit is False: Extracted numeric value or None
        If return_unit is True: Tuple of (numeric value, unit) or (None, None)
    """
    if not text or pd.isna(text):
        return (None, None) if return_unit else None
    
    text = str(text).lower().strip()
    
    # Common unit patterns
    unit_patterns = {
        'kg': ['kg', 'kilo', 'kilogram'],
        'g': ['g', 'gram', 'grams', 'gm'],
        'mg': ['mg', 'milligram'],
        'lb': ['lb', 'pound', 'lbs'],
        'oz': ['oz', 'ounce'],
    }
    
    # Detect unit
    detected_unit = None
    for unit, patterns in unit_patterns.items():
        if any(pattern in text for pattern in patterns):
            detected_unit = unit
            break
    
    # Remove all unit text
    for unit, patterns in unit_patterns.items():
        for pattern in patterns:
            text = text.replace(pattern, '')
    
    # Clean the text
    text = text.replace(',', '.').strip()
    
    # Try direct conversion
    try:
        value = float(text)
        if return_unit:
            return value, detected_unit or 'g'  # Default to grams
        return value
    except ValueError:
        pass
    
    # Try to extract numbers with regex
    try:
        # Look for decimal numbers
        match = re.search(r'(\d+\.?\d*)', text)
        if match:
            value = float(match.group(1))
            
            # Apply unit conversion if needed
            if detected_unit == 'kg':
                value *= 1000
                detected_unit = 'g'
            elif detected_unit == 'mg':
                value /= 1000
                detected_unit = 'g'
            
            if return_unit:
                return value, detected_unit or 'g'
            return value
    except Exception:
        pass
    
    # Last resort - look for ranges and take average
    try:
        range_match = re.search(r'(\d+\.?\d*)\s*[-–—]\s*(\d+\.?\d*)', text)
        if range_match:
            start, end = float(range_match.group(1)), float(range_match.group(2))
            value = (start + end) / 2
            
            # Apply unit conversion if needed
            if detected_unit == 'kg':
                value *= 1000
                detected_unit = 'g'
            elif detected_unit == 'mg':
                value /= 1000
                detected_unit = 'g'
            
            if return_unit:
                return value, detected_unit or 'g'
            return value
    except Exception:
        pass
    
    return (None, None) if return_unit else None


def main():
    """Main function demonstrating ShelfScale workflow with multiple data sources"""
    print("ShelfScale - Comprehensive Food Product Weight Analysis")
    print("=" * 60)
    
    # Set up more detailed logging for debugging
    logging.basicConfig(level=logging.INFO, 
                      format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="ShelfScale - Food Product Weight Analysis")
    parser.add_argument("--output", default="output/weight_dataset.csv", help="Output file path")
    parser.add_argument("--food-portion-pdf", default="D:/LIDA/Product_Weight_Project/Product_Weight_Project_Build/Data/Raw Data/Food_Portion_Sizes.pdf", help="Food Portion Sizes PDF path")
    parser.add_argument("--fruit-veg-pdf", default="D:/LIDA/Product_Weight_Project/Product_Weight_Project_Build/Data/Raw Data/fruit_and_vegetable_survey_2015_sampling_report.pdf", help="Fruit and Veg Survey PDF path")
    parser.add_argument("--mccance-widdowson", default="D:/LIDA/Product_Weight_Project/Product_Weight_Project_Build/Data/Raw Data/McCance_Widdowsons_2021.xlsx", help="McCance and Widdowson data path")
    parser.add_argument("--matching-threshold", type=int, default=70, help="Matching threshold percentage")
    parser.add_argument("--run-dashboard", action="store_true", help="Run interactive dashboard")
    parser.add_argument("--load-cache", action="store_true", help="Load cached data (faster)")
    parser.add_argument("--skip-pdfs", action="store_true", help="Skip PDF extraction (faster)")
    parser.add_argument("--use-super-group", action="store_false", default=True, help="Use Super Group reduced data")
    parser.add_argument("--process-raw", action="store_true", help="Process all raw data and save to Processed directory")
    
    args = parser.parse_args()
    
    # If processing raw data was requested, do it and exit
    if args.process_raw:
        print("Processing all raw data files...")
        processed_data = process_raw_data()
        print("Raw data processing complete. Results saved to Processed directory.")
        return processed_data
    
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(args.output), exist_ok=True)
    
    # Setup data sources
    data_sources = {}
    
    # 1. Load McCance and Widdowson's dataset (highest quality)
    if args.load_cache and os.path.exists("output/mw_data_cached.csv"):
        print("Loading cached McCance and Widdowson data...")
        data_sources["mw_data"] = pd.read_csv("output/mw_data_cached.csv")
    else:
        data_sources["mw_data"] = load_mccance_widdowson_data(args.mccance_widdowson)
    
    # 2. Load McCance and Widdowson's Super Group reduced dataset
    # This contains better structured and cleaned food group data
    super_group_data = {}
    if args.use_super_group:
        print("Loading McCance and Widdowson Super Group reduced data...")
        super_group_dir = "Data/Processed/MW_DataReduction/Reduced Super Group"
        super_group_dir = get_path(super_group_dir)
        if not os.path.isdir(super_group_dir):
            logger.warning(f"Super-group directory '{super_group_dir}' not found – skipping.")
            super_group_files = []
        else:
            super_group_files = [
                f for f in os.listdir(super_group_dir)
                if f.endswith(".csv") and not f.startswith(".")
            ]
        
        # Create output directory if it doesn't exist
        os.makedirs("output", exist_ok=True)
        
        for file in super_group_files:
            file_path = os.path.join(super_group_dir, file)
            try:
                df = pd.read_csv(file_path)
                group_name = os.path.splitext(file)[0]
                super_group_data[group_name] = df
                print(f"  Loaded {len(df)} items from {file}")
            except Exception as e:
                logger.error(f"Error loading {file}: {e}")
        
        # Combine super groups
        if super_group_data:
            data_sources["super_group"] = pd.concat(super_group_data.values())
            print(f"  Total super group items: {len(data_sources['super_group'])}")
    
    # 3. Load Food Portion Sizes data (portion-specific)
    if not args.skip_pdfs:
        try:
            food_portion_path = get_path(args.food_portion_pdf)
            pdf_extractor = PDFExtractor(cache_dir="output")
            data_sources["portion_data"] = pdf_extractor.extract_food_portion_sizes(food_portion_path)
        except FileNotFoundError as e:
            logger.error(f"Could not find food portion PDF: {e}")
            data_sources["portion_data"] = pd.DataFrame()
    elif args.load_cache and os.path.exists("output/food_portion_sizes.csv"):
        print("Loading cached Food Portion Sizes data...")
        data_sources["portion_data"] = pd.read_csv("output/food_portion_sizes.csv")
        print(f"  Loaded {len(data_sources['portion_data'])} items")
    
    # 4. Load Fruit and Vegetable Survey data
    if not args.skip_pdfs:
        try:
            fruit_veg_path = get_path(args.fruit_veg_pdf)
            pdf_extractor = PDFExtractor(cache_dir="output") 
            data_sources["fruit_veg_data"] = pdf_extractor.extract_fruit_veg_survey(fruit_veg_path)
        except FileNotFoundError as e:
            logger.error(f"Could not find fruit and veg PDF: {e}")
            data_sources["fruit_veg_data"] = pd.DataFrame()
    elif args.load_cache and os.path.exists("output/fruit_veg_survey.csv"):
        print("Loading cached Fruit and Vegetable Survey data...")
        data_sources["fruit_veg_data"] = pd.read_csv("output/fruit_veg_survey.csv")
        print(f"  Loaded {len(data_sources['fruit_veg_data'])} items")
    
    # Create an integrated dataset by matching items across sources
    print("\nCreating integrated dataset from all sources...")
    
    # Use MW data as the base dataset
    base_dataset = data_sources.get("mw_data", pd.DataFrame()).copy()
    print(f"Base dataset from McCance and Widdowson: {len(base_dataset)} items")
    
    # Standardize column names for easier matching
    def standardize_columns(df):
        rename_map = {}
        for col in df.columns:
            col_lower = col.lower()
            if 'food' in col_lower and 'name' in col_lower:
                rename_map[col] = 'Food_Name'
            elif col_lower in ['name', 'foodname']:
                rename_map[col] = 'Food_Name'
            elif 'weight' in col_lower:
                rename_map[col] = 'Weight_g'
            elif 'portion' in col_lower and 'size' in col_lower:
                rename_map[col] = 'Portion_Size'
            elif 'food' in col_lower and 'group' in col_lower:
                rename_map[col] = 'Food_Group'
        
        if rename_map:
            return df.rename(columns=rename_map)
        return df
    
    # Apply standardization to base dataset and other sources
    base_dataset = standardize_columns(base_dataset)
    
    # Make sure Food Name is standardized in base dataset
    if "Food Name" in base_dataset.columns and "Food_Name" not in base_dataset.columns:
        base_dataset = base_dataset.rename(columns={"Food Name": "Food_Name"})
    
    # Check required columns
    if "Food_Name" not in base_dataset.columns and "Food Name" in base_dataset.columns:
        base_dataset["Food_Name"] = base_dataset["Food Name"]
    
    # Ensure we have a food name column for matching
    food_name_col = "Food_Name"
    if food_name_col not in base_dataset.columns:
        for col in base_dataset.columns:
            if 'food' in col.lower() and 'name' in col.lower():
                food_name_col = col
                break
            elif 'name' in col.lower():
                food_name_col = col
                break
            elif 'description' in col.lower():
                food_name_col = col
                break
    
    # Initialize matcher
    matcher = FoodMatcher(similarity_threshold=args.matching_threshold / 100)
    
    # Match with Food Portion Sizes data
    portion_data = standardize_columns(data_sources.get("portion_data", pd.DataFrame()).copy())
    print(f"Matching with Food Portion data: {len(portion_data)} items")
    
    if len(portion_data) > 0:
        portion_matches = matcher.match_datasets(
            base_dataset,
            portion_data,
            food_name_col,  # Source column
            "Food_Name" if "Food_Name" in portion_data.columns else "Food Name",  # Target column
            additional_match_cols=None
        )
        
        # Merge matches into base dataset
        merged_portion = matcher.merge_matched_datasets(
            base_dataset,
            portion_data,
            portion_matches,
            None,  # All source columns
            None,  # All target columns
            {"Weight_g": "Portion_Weight_g"}  # Rename target weights
        )
        
        if len(merged_portion) > 0:
            base_dataset = merged_portion
            print(f"Merged with portion data: {len(merged_portion)} items")
        else:
            print("No portion data matches found.")
    
    # Match with Fruit and Vegetable Survey data
    fruit_veg_data = standardize_columns(data_sources.get("fruit_veg_data", pd.DataFrame()).copy())
    print(f"Matching with Fruit and Veg data: {len(fruit_veg_data)} items")
    
    if len(fruit_veg_data) > 0:
        # Make sure the Sample_Name column is used for matching
        sample_name_col = "Sample_Name"
        if sample_name_col not in fruit_veg_data.columns:
            for col in fruit_veg_data.columns:
                if 'sample' in col.lower() and 'name' in col.lower():
                    sample_name_col = col
                    break
        
        fruit_veg_matches = matcher.match_datasets(
            base_dataset,
            fruit_veg_data,
            food_name_col,  # Source column
            sample_name_col,  # Target column
            additional_match_cols=None
        )
        
        # Merge matches into base dataset
        merged_fruit_veg = matcher.merge_matched_datasets(
            base_dataset,
            fruit_veg_data,
            fruit_veg_matches,
            None,  # All source columns
            None,  # All target columns
            {"Pack_Size": "Fruit_Veg_Size"}  # Rename target weights
        )
        
        if len(merged_fruit_veg) > 0:
            base_dataset = merged_fruit_veg
            print(f"Merged with fruit and veg data: {len(merged_fruit_veg)} items")
        else:
            print("No fruit and veg data matches found.")
    
    # Clean the integrated dataset
    print("\nCleaning and formatting the integrated dataset...")
    cleaner = DataCleaner()
    
    try:
        cleaned_dataset = cleaner.clean(base_dataset)
    except Exception as e:
        logger.error(f"Error cleaning dataset: {e}")
        logger.warning("Using original dataset without cleaning.")
        cleaned_dataset = base_dataset
    
    if len(cleaned_dataset) == 0:
        logger.warning("No data available after cleaning. Using original dataset.")
        cleaned_dataset = base_dataset
    
    # Categorize foods by type
    categorizer = FoodCategorizer()
    
    # Find food name column for categorization
    if food_name_col not in cleaned_dataset.columns:
        food_name_col = None
        # Try to find a suitable column for categorization
        for col in cleaned_dataset.columns:
            if 'food' in col.lower() and 'name' in col.lower():
                food_name_col = col
                break
            elif 'name' in col.lower():
                food_name_col = col
                break
            elif 'description' in col.lower():
                food_name_col = col
                break
        
        if not food_name_col:
            logger.warning("Could not find a food name column for categorization.")
            logger.info(f"Available columns:\n{cleaned_dataset.columns.tolist()}")
    
            # Create a dummy column to avoid errors
            if len(cleaned_dataset) > 0:
                cleaned_dataset['Food_Name'] = "Unknown Food"
                food_name_col = 'Food_Name'
    
    print(f"Categorizing foods using column: {food_name_col}")
    
    # Check column existence to avoid errors
    if food_name_col and food_name_col in cleaned_dataset.columns:
        categorized_dataset = categorizer.clean_food_categories(
            cleaned_dataset, 
            food_name_col, 
            'Food_Category', 
            'Super_Category'
        )
    else:
        logger.warning(f"Food name column '{food_name_col}' not found. Skipping categorization.")
        categorized_dataset = cleaned_dataset
        # Add empty category columns
        categorized_dataset['Food_Category'] = "Unknown"
        categorized_dataset['Super_Category'] = "Unknown"
    
    # Process and normalize weight information
    weight_cols = []
    for col in categorized_dataset.columns:
        if 'weight' in col.lower() or 'size' in col.lower():
            weight_cols.append(col)
    
    if weight_cols:
        print(f"Processing weight information from columns: {', '.join(weight_cols)}")
        normalized_dataset = process_weight_info(categorized_dataset, weight_cols)
        print(f"Final dataset has {len(normalized_dataset)} items with weight information")
    else:
        logger.warning("No weight columns found in the dataset")
        normalized_dataset = categorized_dataset
        normalized_dataset['Normalized_Weight'] = np.nan
    
    # Save the comprehensive dataset
    save_data(normalized_dataset, args.output)
    print(f"Saved integrated dataset to {args.output}")
    
    # Launch dashboard if requested
    if args.run_dashboard:
        print("\nLaunching interactive dashboard...")
        try:
            # Only import the dashboard when needed
            from shelfscale.visualization.dashboard import ShelfScaleDashboard
            dashboard = ShelfScaleDashboard(normalized_dataset)
            dashboard.run_server(debug=False, port=8050)
        except ImportError as e:
            logger.error(f"Could not load dashboard. Dashboard dependencies may not be installed: {e}")
            print("Error: Dashboard dependencies not installed. Install with 'pip install dash plotly'.")
    
    print("\nProcess completed successfully!")
    return normalized_dataset


if __name__ == "__main__":
    main() 